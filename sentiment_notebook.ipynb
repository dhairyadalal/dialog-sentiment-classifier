{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis Starter\n",
    "Goal: \n",
    "Develop a fined-tuned sentiment classifier to identify is a provided utterance is positive, neutral, or negative.\n",
    "\n",
    "Background:\n",
    "In general when user interact with Posh's chatbot, they are agnostic in thier responses to the bot. We expect the majorit of user interactions to be overall neutral. However, it is useful to indetify when the user has become visibily frustrated and overall negative. Identifying negative user utterances can help us locate areas where the bot repsonse is insufficient, allows us to develop new emotionally responsive feature, and identify opportunites for escalation.\n",
    "\n",
    "Task description:\n",
    "For this NLP task, we want to map user utterances (u_i) to the following labels [-1,0,1] where -1 is negative, 0 is neutral, and 1 is positive. \n",
    "\n",
    "For example: \n",
    "\n",
    "Utterance: I hate this bot it really doesn't get my question.\n",
    "Expected Label: -1 (negative)\n",
    "\n",
    "Utterance: I am having issues logging in, can you help?\n",
    "Expected Label = 0 (neutral)\n",
    "\n",
    "Utterance: Thanks, your help was great!\n",
    "Expected Label = 1 (positive)\n",
    "\n",
    "Training Data:\n",
    "As a starting point you will be using the ScenarioSA dataset. You can learn more about the dataset here: https://ieeexplore.ieee.org/abstract/document/9091843. ScenarioSA contains 2,214 manually labeled multi-turn English conversations collected from various websites that provide online communication services. We provie some useful script below to get you started, but feel free to explore other avenues.\n",
    "\n",
    "Note: the goal of ScenarioSA is to both classify utterance level sentiments as well as conversation-level sentiments. For simpilicity, the base classifier only considers the former case and ignores the overall context of the conversation. For the first version of this project, we just want to classify utterances in isolation. But if you have time or feel motivated, feel free to explore the conversation-level classification use case. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "['data/sentiment/ScenarioSA/InteractiveSentimentDataset/794.txt',\n 'data/sentiment/ScenarioSA/InteractiveSentimentDataset/2007.txt',\n 'data/sentiment/ScenarioSA/InteractiveSentimentDataset/769.txt',\n 'data/sentiment/ScenarioSA/InteractiveSentimentDataset/1169.txt',\n 'data/sentiment/ScenarioSA/InteractiveSentimentDataset/1454.txt']"
     },
     "metadata": {}
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define path to ScenarioSA dataset\n",
    "path = 'data/sentiment/ScenarioSA'\n",
    "\n",
    "# Generate a list of all files within the ScenarioSA folder\n",
    "files = []\n",
    "# r=root, d=directories, f = files\n",
    "for r, d, f in os.walk(path):\n",
    "    for file in f:\n",
    "        if '.txt' in file:\n",
    "            files.append(os.path.join(r, file))\n",
    "\n",
    "display(files[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                  text  label     source\n0                    I need your help.      0  sentiment\n1                           What's up?      0  sentiment\n2                            I'm lost.     -1  sentiment\n3  Where exactly are you trying to go?      0  sentiment\n4            I want to go see a movie.      0  sentiment",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n      <th>source</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I need your help.</td>\n      <td>0</td>\n      <td>sentiment</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What's up?</td>\n      <td>0</td>\n      <td>sentiment</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I'm lost.</td>\n      <td>-1</td>\n      <td>sentiment</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Where exactly are you trying to go?</td>\n      <td>0</td>\n      <td>sentiment</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I want to go see a movie.</td>\n      <td>0</td>\n      <td>sentiment</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# For each each, extract the individual lines into a global list called rows\n",
    "rows = []\n",
    "for file in files:\n",
    "    with open(file, \"r\", encoding = \"ISO-8859-1\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if len(line) > 4:\n",
    "                ls = line.split()\n",
    "                try:\n",
    "                    label = int(ls[-1])\n",
    "                    line = \" \".join(ls[:-1])\n",
    "                    if \":\" in line:\n",
    "                        line = line.split(\":\")[1].strip()\n",
    "                    rows.append({\"text\": line,\n",
    "                                \"label\": label,\n",
    "                                \"source\": file.split(\"/\")[1]})\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "# Dump rows in to a pandas dataframe\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(rows)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration\n",
    "You may want to explore the dataset here. Some useful analysis topics: distribution of labels, distribution of labels by source, average length of utterances per label, and common words per label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split\n",
    "It is critical when developing classification models to split your training data into train/test/validation segment. Read more here: https://towardsdatascience.com/3-things-you-need-to-know-before-you-train-test-split-869dfabb7e50\n",
    "\n",
    "We can use sklearn's train_test_split method to split our data. Note that random_state argument needs to fixed so that the split is reproducible. We also stratify our test labels to match overall distribution of labels and ensure each label is represented in our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "train size: 38914, test size: 9729\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, \n",
    "                               test_size = .2, \n",
    "                               stratify=df[\"label\"], \n",
    "                               random_state=1988)\n",
    "print(f\"train size: {len(train)}, test size: {len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model \n",
    "\n",
    "Our strong baseline model uses sentence-bert encodings as features for a simple Random Forest Classifier. I'd suggest creating a simple baseline as well using TFIDF features and experinmenting with a multi-nomial logistic regression as a way to better understand how to create a model.\n",
    "\n",
    "*Preprocessing*\n",
    "In this model we use sentence-bert (a customed tuned langauge model for generating sentence level representations - https://github.com/UKPLab/sentence-transformers) to encode our utterances. To intall sentence-bert run the following pip command in your terminal/command line tool: pip install -U sentence-transformers \n",
    "\n",
    "This model has issues distinguishing between negative intents and negative events. For example the utterance \"I broke my laptop, please fix it\" would be classified as negative but the user is actually frustrated just stating the facts. The goal of the updated model is to do a better job discerning between negative events (which should classified as neutral) and negative sentiment (I hate this bot...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('roberta-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = model.encode(train[\"text\"].tolist())\n",
    "y_train = train[\"label\"]\n",
    "print(\"finished encoding X_train inputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfclf = RandomForestClassifier(verbose=True, n_jobs=-1)\n",
    "rfclf.fit(X_train, y_train)\n",
    "print(f\"training accuracy: {rfclf.score(X_train, y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate against Test\n",
    "X_test = model.encode(test[\"text\"].tolist())\n",
    "y_test = test[\"label\"]\n",
    "\n",
    "print(f\"Test accuracy: {rfclf.score(X_test, y_test}\")_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, rfclf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "import pickle\n",
    "\n",
    "pickle.dump(rfclf, open(\"baseline_rf_model.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bitdeeplearningcondac04df1efee874df2ad8ee6cd80464e64",
   "display_name": "Python 3.8.2 64-bit ('deeplearning': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}